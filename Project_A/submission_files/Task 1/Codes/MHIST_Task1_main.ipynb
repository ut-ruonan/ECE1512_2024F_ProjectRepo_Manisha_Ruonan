{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1bbe9f-0fc7-4a0f-b228-3e801b1303e5",
   "metadata": {},
   "source": [
    "# This is the code for the 'Task 1: Dataset Distillation with Attention Matching' Question 2. Dataset Distillation Learning - MHIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551bc51-2f2f-44b4-96e8-2f6c795a0ab5",
   "metadata": {},
   "source": [
    "    ########################################### Code start here for MHIST Dataset ############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b7fcf-95ef-4c4b-8e99-bb50397f28bf",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193afbb4-34f9-4d92-887b-9387c56531ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter\n",
    "!pip install torch torchvision\n",
    "!pip install fvcore\n",
    "!pip install thop\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b626eb-74bc-4c51-8bc6-d89dfc7af80a",
   "metadata": {},
   "source": [
    "# Import Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad706cb9-60a7-4640-8e82-743e7160ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\manis\\\\Documents\\\\ECE digital image processing_Class\\\\Project A')\n",
    "\n",
    "import nbimporter\n",
    "from networks import ConvNet \n",
    "from utils import get_dataset, get_loops, get_eval_pool, evaluate_synset, get_default_convnet_setting, get_network\n",
    "\n",
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "import thop\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbfc3e-73f2-42b5-92fc-8df2dcbae220",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3bcd9-f460-4786-b699-2a718aea0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/mahagam3/Documents/ECE course/Project A'\n",
    "zip_file_path = os.path.join(data_path, 'images.zip')  \n",
    "extract_folder = os.path.join(data_path, 'images')  \n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "print(f'Images extracted to {extract_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fe963-4e4f-4f81-9052-172e7b1a3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/mahagam3/Documents/ECE course/Project A/images/images'\n",
    "annotations_path = os.path.join(data_path, 'annotations.csv') \n",
    "annotations = pd.read_csv(annotations_path)\n",
    "\n",
    "# Resize transformation for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 32x32\n",
    "    transforms.ToTensor()  # Convert to tensor\n",
    "])\n",
    "\n",
    "class MHISTDataset(Dataset):\n",
    "    def __init__(self, annotations, data_path, partition, transform=None):\n",
    "        self.annotations = annotations[annotations['Partition'] == partition]  # Filter based on partition\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform  # Use the transform passed in\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.annotations.iloc[idx]['Image Name']  # Get image name\n",
    "        label = self.annotations.iloc[idx]['Majority Vote Label']  # Get label from the updated column\n",
    "        label = 0 if label == 'HP' else 1 \n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(os.path.join(self.data_path, img_name)).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transformation if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Create datasets for training and testing\n",
    "mhist_train = MHISTDataset(annotations, data_path, partition='train', transform=transform)\n",
    "mhist_test = MHISTDataset(annotations, data_path, partition='test', transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "mhist_loader = DataLoader(mhist_train, batch_size=64, shuffle=True)\n",
    "mhist_test_loader = DataLoader(mhist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be2537-8323-4d05-86e8-690365ca69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = annotations['Majority Vote Label'].value_counts()\n",
    "print(\"Number of images per class:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022fa7a-730f-40ca-a78e-2caefcc8d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Edit to MHIST ###################\n",
    " ###### MHIST Dataset ######\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize a counter for the classes\n",
    "class_counts = Counter()\n",
    "\n",
    "# Iterate through the training dataset\n",
    "for _, label in mhist_train:\n",
    "    class_counts[label] += 1  # Increment the count for the corresponding class\n",
    "\n",
    "# Print the number of images per class\n",
    "for class_label, count in class_counts.items():\n",
    "    print(f\"Class {class_label}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf8457-01c2-4951-b2e4-7a1ddde3bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for MHIST ConvNet7\n",
    "channel_mhist = 3  \n",
    "num_classes_mhist = 2 \n",
    "im_size_mhist = (224, 224)\n",
    "\n",
    "# Resize MHIST images\n",
    "transform_resize = transforms.Compose([\n",
    "    transforms.Resize(im_size_mhist),  \n",
    "    transforms.ToTensor() \n",
    "])\n",
    "\n",
    "# Instantiate ConvNetD7\n",
    "convnet7 = get_network('ConvNetD7', channel_mhist, num_classes_mhist, im_size_mhist).to('cpu')\n",
    "\n",
    "optimizer7 = optim.SGD(convnet7.parameters(), lr=0.01) # Define optimizer\n",
    "criterion = nn.CrossEntropyLoss() # Define loss function\n",
    "scheduler7 = CosineAnnealingLR(optimizer7, T_max=20) # Cosine Annealing Scheduler\n",
    "\n",
    "# Create datasets for training and testing\n",
    "mhist_train = MHISTDataset(annotations, data_path, partition='train', transform=transform)\n",
    "mhist_test = MHISTDataset(annotations, data_path, partition='test', transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "mhist_loader = DataLoader(mhist_train, batch_size=64, shuffle=True)\n",
    "mhist_test_loader = DataLoader(mhist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c232f186-bd11-4584-8249-1196c84a50a0",
   "metadata": {},
   "source": [
    "# Part 1 Question 2(a):\n",
    "Train the selected model with the original dataset and report the classification accuracy along  with floating-point operations per second (FLOPs) for the test set. Use SGD as an optimizer\n",
    "with a cosine annealing scheduler with an initial learning rate of 0.01 for 20 epochs. (For more information on experimental setting, look at the implementation details of [51]) These scores give you the upper bound benchmark evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba940094-62fc-496d-b9ed-04f2f0b7e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, dataloader, optimizer, criterion, scheduler, num_epochs=20):\n",
    "    model.train()  # Set the model to training mode\n",
    "    start_time = time.time()  # Start timing\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to device\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize the model\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "\n",
    "    total_training_time = time.time() - start_time  # Calculate total time\n",
    "    print(f\"Training Time on Real MHIST Dataset: {total_training_time:.2f} seconds\")\n",
    "    return total_training_time\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e0794-da50-4192-8537-d91c50fa358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ConvNet7 on MHIST\n",
    "print(\"Training ConvNet7 on MHIST...\")\n",
    "train_model(convnet7, mhist_loader, optimizer7, criterion, scheduler7)\n",
    "\n",
    "# Evaluate ConvNet7 on test set\n",
    "accuracy_mhist = evaluate_model(convnet7, mhist_test_loader)\n",
    "print(f'ConvNet7 Test Accuracy on MHIST: {accuracy_mhist:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93c38e-b53f-4cc3-b345-e2cab3c2aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flop is used to measure the coputational cost of running the model and to evaluate the efficiency of the learning model.\n",
    "# Calculate FLOPs (Floating Point Operations per Second)\n",
    "def calculate_flops(model, input_tensor):\n",
    "    from thop import profile\n",
    "    \n",
    "    # Clear any existing 'total_ops' and 'total_params' attributes\n",
    "    for layer in model.modules():\n",
    "        if hasattr(layer, 'total_ops'):\n",
    "            del layer.total_ops  # Remove existing 'total_ops'\n",
    "        if hasattr(layer, 'total_params'):\n",
    "            del layer.total_params  # Remove existing 'total_params'\n",
    "\n",
    "    # Now calculate FLOPs\n",
    "    flops, params = profile(model, inputs=(input_tensor,))\n",
    "    return flops\n",
    "\n",
    "# Input tensor for calculating FLOPs\n",
    "input_tensor_mhist = torch.randn(len(mhist_train), channel_mhist, 224, 224)\n",
    "flops_mhist = calculate_flops(convnet7, input_tensor_mhist)\n",
    "print(f'FLOPs for ConvNet7: {flops_mhist:.2f} FLOPs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ed8e3-5b42-4edd-b2b8-0888913b1c82",
   "metadata": {},
   "source": [
    "# Part 1: Question 2(b): \n",
    "Learn the synthetic dataset S using the selected model and Attention Matching algorithm. For initialization of condensed images, randomly select from real training images. The experimental setup can be found in Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889974d-c5eb-4462-baa5-c6a319148053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for MHIST\n",
    "K_mhist = 200\n",
    "T_mhist = 10\n",
    "eta_S_mhist = 0.1\n",
    "zeta_S_mhist = 1\n",
    "eta_theta_mhist = 0.01\n",
    "zeta_theta_mhist = 50\n",
    "lambda_mhist = 0.01\n",
    "num_classes_mhist = 2\n",
    "images_per_class_mhist = 50\n",
    "batch_size_mhist = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c755d-a06d-4011-bbc1-4402d0763c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a synthetic dataset S by randomly sampling from a real dataset\n",
    "def generate_synthetic_dataset(real_dataset, num_classes, images_per_class):\n",
    "    synthetic_images = []\n",
    "    synthetic_labels = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        indices = random.sample(\n",
    "            [i for i, (_, label) in enumerate(real_dataset) if label == class_id],\n",
    "            images_per_class\n",
    "        )\n",
    "        synthetic_images.extend([real_dataset[i][0] for i in indices])\n",
    "        synthetic_labels.extend([real_dataset[i][1] for i in indices])\n",
    "    \n",
    "    return synthetic_images, synthetic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a589547-cc3a-40a5-9604-181728cfebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic samples for MHIST\n",
    "synthetic_images_mhist, synthetic_labels_mhist = generate_synthetic_dataset(\n",
    "    mhist_train, num_classes_mhist, images_per_class_mhist\n",
    ")\n",
    "\n",
    "# Convert synthetic images to a tensor\n",
    "synthetic_images_tensor_mhist = torch.stack(synthetic_images_mhist).to(device)\n",
    "synthetic_labels_tensor_mhist = torch.tensor(synthetic_labels_mhist).to(device)\n",
    "\n",
    "# Create DataLoader for the synthetic MHIST dataset\n",
    "synthetic_dataset_mhist = torch.utils.data.TensorDataset(synthetic_images_tensor_mhist, synthetic_labels_tensor_mhist)\n",
    "synthetic_loader_mhist = DataLoader(synthetic_dataset_mhist, batch_size=batch_size_mhist, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7bf93e-39d1-4664-81cb-b8f12c7cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Save the synthetic images ########\n",
    "# Save synthetic images as PNG files\n",
    "output_dir = \"synthetic_images_mhist\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save each synthetic image as a PNG file\n",
    "for idx, synthetic_image in enumerate(synthetic_images_mhist):\n",
    "    image_path = os.path.join(output_dir, f\"synthetic_image_{idx}.png\")\n",
    "    save_image(synthetic_image, image_path)\n",
    "\n",
    "print(f\"Synthetic images saved in {output_dir}\")\n",
    "\n",
    "# Save synthetic images and labels as a .pt file\n",
    "torch.save({\n",
    "    'images': synthetic_images_tensor_mhist,\n",
    "    'labels': synthetic_labels_tensor_mhist\n",
    "}, \"synthetic_mhist.pt\")\n",
    "\n",
    "print(\"Synthetic images and labels saved as 'synthetic_mhist.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dbc6e-77fb-4391-a79e-e8f9e57eb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_module import get_attention\n",
    "# Training Function with Attention Matching\n",
    "def train_with_attention_matching(model, synthetic_dataloader, optimizer, criterion, \n",
    "                                  param=0, exp=4, num_epochs=10, lambda_param=0.01):\n",
    "    model.train()\n",
    "    start_time = time.time()  # Start timing\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in synthetic_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass to get features\n",
    "            features = model(images)\n",
    "            # Get attention maps\n",
    "            attention_maps = get_attention(features, param=param, exp=exp)\n",
    "            \n",
    "            # Compute loss with attention matching\n",
    "            loss = criterion(features, labels)\n",
    "            # Incorporate task balance parameter Î»\n",
    "            loss += lambda_param * torch.mean(attention_maps)  \n",
    "            \n",
    "            loss.backward()  \n",
    "            optimizer.step() \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(synthetic_dataloader):.4f}')\n",
    "        total_training_time = time.time() - start_time  # Calculate total time\n",
    "    print(f\"Training Time on Real MHIST Dataset: {total_training_time:.2f} seconds\")\n",
    "    return total_training_time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optimizer setup for the model (using SGD as per the table)\n",
    "optimizer_mhist = torch.optim.SGD(convnet7.parameters(), lr=eta_theta_mhist)\n",
    "convnet7 = get_network('ConvNetD7', channel_mhist, num_classes_mhist, im_size_mhist).to(device)\n",
    "\n",
    "# Train the model using the synthetic datasets, cross_entropy measure how well the model predicts the classes.\n",
    "print(\"Training on MHIST dataset...\")\n",
    "train_with_attention_matching(convnet7, synthetic_loader_mhist, optimizer_mhist, F.cross_entropy, \n",
    "                              param=0, exp=4, num_epochs=T_mhist, lambda_param=lambda_mhist)\n",
    "\n",
    "# # Evaluate ConvNet7 on the test set\n",
    "# accuracy_attention_mhist = evaluate_model(convnet7, mhist_test_loader)\n",
    "# print(f'ConvNet7 Test Accuracy on MHIST: {accuracy_attention_mhist:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4adb47-ded1-44f6-81a2-045b7b8f0176",
   "metadata": {},
   "source": [
    "# Part 1: Question 2(c):\n",
    "Provide the visualization of condensed images per class for both MNIST and MHIST datasets. \n",
    "Do you think these condensed images are recognizable? Support your explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38857e29-2f33-4f6d-ba55-aa747c4b16ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_condensed_images(synthetic_images, num_classes, images_per_class, title, save_folder):\n",
    "    fig, axes = plt.subplots(num_classes, images_per_class, figsize=(images_per_class * 2, num_classes * 2))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        for img_id in range(images_per_class):\n",
    "            idx = class_id * images_per_class + img_id\n",
    "            # Permute the tensor to make it compatible with imshow (from CxHxW to HxWxC)\n",
    "            img = synthetic_images[idx].permute(1, 2, 0).cpu().numpy()\n",
    "            axes[class_id, img_id].imshow(img)\n",
    "            axes[class_id, img_id].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_folder}/{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "save_folder = 'Save_images'\n",
    "\n",
    "# visualize MHIST when data is available\n",
    "visualize_condensed_images(synthetic_images_tensor_mhist, num_classes_mhist, images_per_class_mhist, \n",
    "                           title=\"Condensed Images for MHIST\", save_folder=save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587867a-4468-40c6-ba71-28ba2536d6bf",
   "metadata": {},
   "source": [
    "# Part 1 Question 2(d):\n",
    "Repeat parts 2b and 2c while the condensed images are initialized with Gaussian noise. Discuss in full detail the qualitative and quantitative results you have achieved. Are the results and visualizations are comparable with parts 2b and 2c?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c3b6f-731e-4b47-9ce0-642955b26630",
   "metadata": {},
   "source": [
    "# Repeat parts 2(b) while the condensed images are initialized with Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d617b0-2a59-40c8-b584-455b4ef0ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic dataset S with Gaussian noise\n",
    "# Reduce the standard deviation to make the noise less pronounced, leads to less spread in the distribution,......\n",
    "# meaning values stay closer to the mean. This results in less noise.\n",
    "def generate_synthetic_dataset_with_noise(real_dataset, num_classes, images_per_class, noise_std=0.8):\n",
    "    synthetic_images = []\n",
    "    synthetic_labels = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        indices = random.sample(\n",
    "            [i for i, (_, label) in enumerate(real_dataset) if label == class_id],\n",
    "            images_per_class\n",
    "        )\n",
    "        for i in indices:\n",
    "            # Generate Gaussian noise\n",
    "            noise = torch.normal(mean=0, std=noise_std, size=real_dataset[i][0].size())\n",
    "            synthetic_image = real_dataset[i][0] + noise\n",
    "            \n",
    "            # Ensure the pixel values are within valid range\n",
    "            synthetic_image = torch.clamp(synthetic_image, 0, 1)\n",
    "            \n",
    "            synthetic_images.append(synthetic_image)\n",
    "            synthetic_labels.append(real_dataset[i][1])\n",
    "    \n",
    "    return synthetic_images, synthetic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fce55e-a107-4b44-b7cc-e6bc03444cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating synthetic samples for MHIST\n",
    "synthetic_images_mhist_noise, synthetic_labels_mhist_noise = generate_synthetic_dataset_with_noise(\n",
    "    mhist_train, num_classes_mhist, images_per_class_mhist\n",
    ") \n",
    "# Convert synthetic images to a tensor\n",
    "synthetic_images_tensor_mhist_noise = torch.stack(synthetic_images_mhist_noise).to(device)\n",
    "synthetic_labels_tensor_mhist_noise = torch.tensor(synthetic_labels_mhist_noise).to(device)\n",
    "\n",
    "# Create DataLoader for the synthetic MHIST dataset with noise\n",
    "synthetic_dataset_mhist_noise = torch.utils.data.TensorDataset(synthetic_images_tensor_mhist_noise, synthetic_labels_tensor_mhist_noise)\n",
    "synthetic_loader_mhist_noise = DataLoader(synthetic_dataset_mhist_noise, batch_size=batch_size_mhist, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d637320-88af-4282-87d2-bf44253f4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Save the synthetic images with noise ########\n",
    "# Save synthetic images as PNG files\n",
    "output_dir = \"synthetic_images_mhist_noise\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save each synthetic image as a PNG file\n",
    "for idx, synthetic_image in enumerate(synthetic_images_mhist_noise):\n",
    "    image_path = os.path.join(output_dir, f\"synthetic_image_{idx}.png\")\n",
    "    save_image(synthetic_image, image_path)\n",
    "\n",
    "print(f\"Synthetic images saved in {output_dir}\")\n",
    "\n",
    "# Save synthetic images and labels as a .pt file\n",
    "torch.save({\n",
    "    'images': synthetic_images_tensor_mhist_noise,\n",
    "    'labels': synthetic_labels_tensor_mhist_noise\n",
    "}, \"synthetic_mhist_noise.pt\")\n",
    "\n",
    "print(\"Synthetic images and labels saved as 'synthetic_mhist_noise.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef36703-1cbe-4c4b-8bb6-8073e1dd408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function with Attention Matching\n",
    "def train_with_attention_matching(model, synthetic_dataloader, optimizer, criterion, \n",
    "                                  param=0, exp=4, num_epochs=10, lambda_param=0.01):\n",
    "    model.train()\n",
    "    start_time = time.time()  # Start timing\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in synthetic_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass to get features\n",
    "            features = model(images)\n",
    "            # Get attention maps\n",
    "            attention_maps = get_attention(features, param=param, exp=exp)\n",
    "            \n",
    "            # Compute loss with attention matching\n",
    "            loss = criterion(features, labels)\n",
    "            # Incorporate task balance parameter lambda\n",
    "            loss += lambda_param * torch.mean(attention_maps)  # Adjust this based on your needs\n",
    "            \n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize the model\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(synthetic_dataloader):.4f}')\n",
    "        total_training_time = time.time() - start_time  # Calculate total time\n",
    "    print(f\"Training Time on Real MHIST Dataset: {total_training_time:.2f} seconds\")\n",
    "    return total_training_time\n",
    "\n",
    "# Optimizer setup for the model (using SGD as per the table)\n",
    "optimizer_mhist_noise = torch.optim.SGD(convnet7.parameters(), lr=eta_theta_mhist)\n",
    "\n",
    "# Train the model using the synthetic datasets with Gaussian noise\n",
    "print(\"Training on MHIST dataset with Gaussian noise...\")\n",
    "train_with_attention_matching(convnet7, synthetic_loader_mhist_noise, optimizer_mhist_noise, \n",
    "                              F.cross_entropy, param=0, exp=4, num_epochs=T_mhist, lambda_param=lambda_mhist)\n",
    "\n",
    "# # Evaluate ConvNet7 on the test set\n",
    "# accuracy_attention_mhist_noise = evaluate_model(convnet7, mhist_test_loader)\n",
    "# print(f'ConvNet7 Test Accuracy on MHIST: {accuracy_attention_mhist_noise:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffdedd-ac7b-44eb-aaac-7834b683715a",
   "metadata": {},
   "source": [
    "# Repeat parts 2(c) while the condensed images are initialized with Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c367e19-de32-4f37-9b10-dec0884be06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_condensed_images(synthetic_images_tensor, num_classes, images_per_class, save_dir, title):\n",
    "    plt.figure(figsize=(images_per_class * 2, num_classes * 2))\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        class_images = synthetic_images_tensor[class_id * images_per_class: (class_id + 1) * images_per_class]\n",
    "        for i, image in enumerate(class_images):\n",
    "            plt.subplot(num_classes, images_per_class, class_id * images_per_class + i + 1)\n",
    "            # Permute the tensor to match (H, W, C) format and convert to numpy\n",
    "            img = image.permute(1, 2, 0).cpu().detach().numpy()\n",
    "            plt.imshow(img) \n",
    "            plt.axis('off')\n",
    "            \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f\"{title.replace(' ', '_')}.png\")  # Define the path where the image will be saved\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    print(f\"Image saved at: {save_path}\")\n",
    "    \n",
    "# Visualize and save condensed images for MHIST with Gaussian noise\n",
    "visualize_condensed_images(synthetic_images_tensor_mhist_noise, num_classes_mhist, images_per_class_mhist, \n",
    "                           save_dir=\"Save_images\", title=\"Condensed Images for MHIST with Gaussian Noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64fcda-9c67-4a1a-a529-58bb6e0c13dd",
   "metadata": {},
   "source": [
    "# Part 1 Question 2(e):\n",
    "Now that you have had a chance to understand, learn, and visualize the condensed dataset, we can train the selected network from scratch on the condensed images. Train the selected network on a learned synthetic dataset (with 100 training images), then evaluate it on the\n",
    "real testing data. Compare the test accuracy performance and the training time with part 2a. Explain your results. (For a fair comparison, you should use the exact same experimental setting as part 2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50f4e5-acc5-464e-8bfa-3ec211be7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model on the synthetic condensed dataset\n",
    "def train_model_on_synthetic(model, dataloader, optimizer, criterion, scheduler, num_epochs=20):\n",
    "    model.train()  \n",
    "    start_time = time.time()  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize the model\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "    \n",
    "    total_training_time = time.time() - start_time\n",
    "    print(f\"Training Time on Condensed Dataset: {total_training_time:.2f} seconds\")\n",
    "    return total_training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62884278-36fc-4ae1-95bb-fab0a83b3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the condensed synthetic dataset (Part 2e)\n",
    "print(\"\\nTraining ConvNet7 on Condensed Synthetic Dataset...\")\n",
    "training_time_condensed = train_model_on_synthetic(convnet7, synthetic_loader_mhist, optimizer7, criterion, scheduler7)\n",
    "print(f\"Total Training Time on Condensed Dataset: {training_time_condensed:.2f} seconds\")\n",
    "\n",
    "# Evaluate model on the real test data (MHIST test set). This is the same evaluation model 'ConvNet7' applied to MHIST data in 2(a)\n",
    "accuracy_synthetic_mhist = evaluate_model(convnet7, mhist_test_loader)\n",
    "print(f'ConvNet7 Test Accuracy on Real MHIST after training on condensed dataset: {accuracy_synthetic_mhist:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cca4f-a14b-4c23-80b7-d678a05cae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor for calculating FLOPs\n",
    "input_tensor_mhist = torch.randn(100, channel_mhist, 224, 224)\n",
    "flops_mhist = calculate_flops(convnet7, input_tensor_mhist)\n",
    "print(f'FLOPs for ConvNet7: {flops_mhist:.2f} FLOPs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7284133-9734-4f9a-9e56-ac0a8b0aa874",
   "metadata": {},
   "source": [
    "# Part 1 Question 3: Cross-architecture Generalization\n",
    "The ResNet18 is used in this section from Resnet18_mhist.py file to evaluate its cross-architecture performance in terms of classification accuracy\n",
    "on the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fedb76-7230-46a1-a92b-6b63efd229e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resnet18_mhist import ResNet18\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define parameters\n",
    "model_name = 'ResNet18' \n",
    "channel = 3              \n",
    "num_classes = 2          \n",
    "im_size = (224, 224)        \n",
    "\n",
    "transform_resize = transforms.Compose([\n",
    "    transforms.Resize(im_size),  \n",
    "    transforms.ToTensor()  \n",
    "])\n",
    "\n",
    "# Get the network instance using the ResNet18 function\n",
    "resnet18 = ResNet18(channel=channel, num_classes=num_classes)\n",
    "\n",
    "# Prepare the condensed MHIST dataset\n",
    "synthetic_dataset_mhist = TensorDataset(torch.stack(synthetic_images_mhist), torch.tensor(synthetic_labels_mhist))\n",
    "synthetic_loader_mhist = DataLoader(synthetic_dataset_mhist, batch_size=batch_size_mhist, shuffle=True)\n",
    "\n",
    "# Define optimizer, criterion, and scheduler\n",
    "optimizer_mhist = torch.optim.Adam(resnet18.parameters(), lr=0.001)\n",
    "criterion_mhist = torch.nn.CrossEntropyLoss()\n",
    "scheduler_mhist = torch.optim.lr_scheduler.StepLR(optimizer_mhist, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train ResNet18 on the condensed MHIST dataset\n",
    "print(\"\\nTraining ResNet18 on Condensed MHIST Dataset...\")\n",
    "training_time_mhist = train_model_on_synthetic(resnet18, synthetic_loader_mhist, optimizer_mhist, criterion_mhist, scheduler_mhist)\n",
    "\n",
    "# Evaluate on the real MHIST test set\n",
    "mhist_test_loader = DataLoader(mhist_test, batch_size=batch_size_mhist, shuffle=False)\n",
    "mhist_accuracy = evaluate_model(resnet18, mhist_test_loader)\n",
    "print(f'ResNet18 Test Accuracy on Real MHIST: {mhist_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94978f78-8e45-41af-80bd-1ef3815133cc",
   "metadata": {},
   "source": [
    "          ############################# Code ends here for MHIST Dataset  ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930db207-32e3-48b1-8f91-08a3e8c9ffa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
