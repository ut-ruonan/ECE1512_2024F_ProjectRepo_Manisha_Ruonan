{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a8f5527ccc8190"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Basic Concepts\n",
    "1. The purpose of using dataset distillation in this paper is to reduce the training costs while maintaining the high performance on various machine learning tasks. The authors introduce Dataset Distillation with Attention Maching (DataDAM) to condense large datasets into smaller synthetic dataset that retain the critical information, allowing models trained on the synthetic set to achieve similar accuracy as those trained on the full dataset.\n",
    "2.  The advantages are: (page 2)\n",
    "- Efficient end-to-end dataset distillation: This highlights the ability of DataDAM to closely approximate the distribution of the real dataset while keeping **computational costs low**.\n",
    "- Improved accuracy and scalability: DataDAM demonstrate the performance across multiple benchmark dataset and reduces the training costs by up to 100x, while also allowing for cross-architecture generation. This makes it more scalable and flexible for real-world application.\n",
    "- Enhancement of downstream application: DATADAM's distilled data improves memory efficiency in continual learning tasks and accelerates neural architectures search(NAS) by providing a more representative proxy dataset, enabling a faster and more efficient learning process.  \n",
    "3. The novelty includes: (page 2)\n",
    "- Multiple Randomly Initialized DNNs: DataDAM uses multiple randomly initialized deep neural networks to extract meaningful representations from both real and synthetic datasets, which is different from methods that rely on pre-trained models\n",
    "- Spatial attention matching (SAM): The SAM module align the most discriminative feature maps from real and synthetic datasets, reducing the gap between the dataset.\n",
    "- Last-Layer Feature Alignment: It reduces disparities in the last-layer feature distributions between the real and synthetic datasets by using a complementary loss as a regularizer, ensuring high-level abstract representations are similar.\n",
    "- Bias-Free Synthetic Data: The synthetic data generated by DataDAM does not introduce any bias, which is a significant improvement over prior methods, ensuring better generalization and performance.\n",
    "4. The methodology of DataDAM is centered on efficiently distilling datasets through attention matching: (page 4)\n",
    "- Initialization of Synthetic Dataset: The process starts by initializing a synthetic dataset, which can be done through random noise or by sampling real data.\n",
    "- Feature Extraction: Real and synthetic datasets are passed through randomly initialized deep neural networks, and features are extracted at multiple layers.\n",
    "- Spatial Attention Matching (SAM): Attention maps are computed for each layer, excluding the final layer. These attention maps focus on the most discriminative regions of the input image. \n",
    "- Loss Functions:\n",
    "    - SAM Loss (LSAM): This loss minimizes the distance between attention maps of real and synthetic datasets across layers.\n",
    "    - Maximum Mean Discrepancy Loss (LMMD): This complementary loss aligns the last-layer feature distributions of the two datasets, ensuring the high-level abstract information is captured.\n",
    "- Optimization: The synthetic dataset is optimized using a combination of the SAM loss and LMMD loss to minimize the difference between real and synthetic data.\n",
    "5. (page 8)\n",
    "- Continual Learning: DataDAMâ€™s ability to condense datasets efficiently makes it highly useful in continual learning scenarios, where a model must learn incrementally while preventing catastrophic forgetting. By using the distilled datasets as a replay buffer, DataDAM can significantly improve memory efficiency and performance in incremental learning tasks.\n",
    "- Neural Architecture Search (NAS): The synthetic datasets generated by DataDAM can serve as proxies in NAS tasks, allowing faster evaluation of model architectures. This leads to a significant reduction in computational costs and time during the model search process, making NAS more feasible in real-world applications."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b22ea2313b253dbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Data Distillation Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6968bdcc4e8c439"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import shutil \n",
    "import os \n",
    "\n",
    "annotations = pd.read_csv('mhist_dataset/annotations.csv')\n",
    "\n",
    "img_folder = 'mhist_dataset/images'\n",
    "train_folder = 'mhist_dataset/train/'\n",
    "test_folder = 'mhist_dataset/test/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:44:37.153173Z",
     "start_time": "2024-10-16T01:44:36.199688Z"
    }
   },
   "id": "7070b2acb6eef704",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have been successfully moved\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "for index, row in annotations.iterrows():\n",
    "    img_name = row['Image Name']\n",
    "    partition = row['Partition']\n",
    "    \n",
    "    src_path = os.path.join(img_folder, img_name)\n",
    "    if partition == 'train':\n",
    "        dst_path = os.path.join(train_folder, img_name)\n",
    "    else:\n",
    "        dst_path = os.path.join(test_folder, img_name)\n",
    "    \n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Images have been successfully moved\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:33:04.806730Z",
     "start_time": "2024-10-16T01:33:00.708942Z"
    }
   },
   "id": "56113c0d7dc783da",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been moved correctly and counts match!\n"
     ]
    }
   ],
   "source": [
    "actual_train_count = len(os.listdir(train_folder))\n",
    "actual_test_count = len(os.listdir(test_folder))\n",
    "\n",
    "expected_train_count = len(annotations[annotations['Partition'] == 'train'])\n",
    "expected_test_count = len(annotations[annotations['Partition'] == 'test'])\n",
    "\n",
    "if actual_train_count == expected_train_count and actual_test_count == expected_test_count:\n",
    "    print(\"All files have been moved correctly and counts match!\")\n",
    "else:\n",
    "    print(\"Warning: There is a mismatch between the files moved and the expected count!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:33:04.817588Z",
     "start_time": "2024-10-16T01:33:04.803827Z"
    }
   },
   "id": "4b90604094e2cca0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%run networks.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:46:22.534409Z",
     "start_time": "2024-10-16T01:46:22.196732Z"
    }
   },
   "id": "d2605b3f8254c00e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/2tnxyshs57qck1x9ynk2d0dr0000gn/T/ipykernel_79488/3964166146.py:9: DeprecationWarning: Please import `rotate` from the `scipy.ndimage` namespace; the `scipy.ndimage.interpolation` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.ndimage.interpolation import rotate as scipyrotate\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networks'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m/var/folders/vf/2tnxyshs57qck1x9ynk2d0dr0000gn/T/ipykernel_79488/3964166146.py:10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets, transforms\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mndimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minterpolation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rotate \u001B[38;5;28;01mas\u001B[39;00m scipyrotate\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnetworks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MLP, ConvNet, LeNet, AlexNet, AlexNetBN, VGG11, VGG11BN, ResNet18, ResNet18BN_AP, ResNet18BN\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dataset\u001B[39m(dataset, data_path):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dataset \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMNIST\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'networks'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networks'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutils.ipynb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ECE1512_2024F_ProjectRepo_Manisha_Ruonan/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2480\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2478\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[1;32m   2479\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2480\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2482\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2483\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2484\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[0;32m~/Desktop/ECE1512_2024F_ProjectRepo_Manisha_Ruonan/.venv/lib/python3.11/site-packages/IPython/core/magics/execution.py:737\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    735\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m preserve_keys(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39muser_ns, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__file__\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    736\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39muser_ns[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__file__\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m filename\n\u001B[0;32m--> 737\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msafe_execfile_ipy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    740\u001B[0m \u001B[38;5;66;03m# Control the response to exit() calls made by the script being run\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ECE1512_2024F_ProjectRepo_Manisha_Ruonan/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3005\u001B[0m, in \u001B[0;36mInteractiveShell.safe_execfile_ipy\u001B[0;34m(self, fname, shell_futures, raise_exceptions)\u001B[0m\n\u001B[1;32m   3003\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_cell(cell, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, shell_futures\u001B[38;5;241m=\u001B[39mshell_futures)\n\u001B[1;32m   3004\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m raise_exceptions:\n\u001B[0;32m-> 3005\u001B[0m     \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3006\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result\u001B[38;5;241m.\u001B[39msuccess:\n\u001B[1;32m   3007\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ECE1512_2024F_ProjectRepo_Manisha_Ruonan/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:308\u001B[0m, in \u001B[0;36mExecutionResult.raise_error\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_before_exec\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_in_exec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 308\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_in_exec\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m/var/folders/vf/2tnxyshs57qck1x9ynk2d0dr0000gn/T/ipykernel_79488/3964166146.py:10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets, transforms\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mndimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minterpolation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rotate \u001B[38;5;28;01mas\u001B[39;00m scipyrotate\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnetworks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MLP, ConvNet, LeNet, AlexNet, AlexNetBN, VGG11, VGG11BN, ResNet18, ResNet18BN_AP, ResNet18BN\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dataset\u001B[39m(dataset, data_path):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dataset \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMNIST\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'networks'"
     ]
    }
   ],
   "source": [
    "%run utils.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:46:30.592405Z",
     "start_time": "2024-10-16T01:46:28.496670Z"
    }
   },
   "id": "cf3727d31f4de936",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networks'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Users/ruby/Desktop/ECE1512_2024F_ProjectRepo_Manisha_Ruonan/Project_A/submission_files\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Try importing your networks or other modules\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnetworks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MLP, ConvNet\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'networks'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/Users/ruby/Desktop/ECE1512_2024F_ProjectRepo_Manisha_Ruonan/Project_A/submission_files')\n",
    "\n",
    "# Try importing your networks or other modules\n",
    "from networks import MLP, ConvNet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:46:26.163593Z",
     "start_time": "2024-10-16T01:46:26.140309Z"
    }
   },
   "id": "cd1b7e3c83b56c27",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_path = 'mnist_dataset'\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "train_data, test_data, class_names = get_dataset('MNIST', './data')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d76bf159e8c71ac"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'SymInt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SGD\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataLoader\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datasets, transforms\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/__init__.py:6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodulefinder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Module\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchvision\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mextension\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/_meta_registrations.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_custom_ops\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlibrary\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Ensure that torch.ops.torchvision is visible\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_custom_ops.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_custom_op\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimpl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      4\u001B[0m     _custom_op_with_schema,\n\u001B[1;32m      5\u001B[0m     _find_custom_op,\n\u001B[1;32m      6\u001B[0m     infer_schema,\n\u001B[1;32m      7\u001B[0m     parse_qualname,\n\u001B[1;32m      8\u001B[0m     validate_namespace,\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlibrary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_ctx\n\u001B[1;32m     12\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom_op\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimpl\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimpl_backward\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     19\u001B[0m ]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_custom_op/impl.py:13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_C\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_C\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlibrary\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mlibrary\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_library\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabstract_impl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AbstractImplCtx\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlibrary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_ctx\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autograd_kernel_indirection, construct_autograd_kernel\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_library/__init__.py:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_library\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabstract_impl\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_library\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msimple_registry\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_library\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_library/abstract_impl.py:117\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    114\u001B[0m         global_ctx_getter \u001B[38;5;241m=\u001B[39m prev\n\u001B[0;32m--> 117\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mAbstractImplCtx\u001B[39;00m:\n\u001B[1;32m    118\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;124;03m    Context object for writing abstract implementations for custom operators.\u001B[39;00m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, _shape_env, _op):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_library/abstract_impl.py:126\u001B[0m, in \u001B[0;36mAbstractImplCtx\u001B[0;34m()\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape_env \u001B[38;5;241m=\u001B[39m _shape_env\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_op \u001B[38;5;241m=\u001B[39m _op\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_unbacked_symint\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, \u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mSymInt:\n\u001B[1;32m    127\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    128\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreate_unbacked_symint is deprecated, please use new_dynamic_size instead\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    129\u001B[0m     )\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnew_dynamic_size(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmin\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'torch' has no attribute 'SymInt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:15:53.472218Z",
     "start_time": "2024-10-16T01:15:53.285044Z"
    }
   },
   "id": "4efd17a8d3f78422",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m      2\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor()\n\u001B[1;32m      3\u001B[0m ])\n\u001B[1;32m      5\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mImageFolder(root\u001B[38;5;241m=\u001B[39mtrain_folder, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[1;32m      6\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mImageFolder(root\u001B[38;5;241m=\u001B[39mtest_folder, transform\u001B[38;5;241m=\u001B[39mtransform)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_folder, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_folder, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T00:55:57.254750Z",
     "start_time": "2024-10-16T00:55:57.115373Z"
    }
   },
   "id": "5931daa181a604e8",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize ConvNet-3 for MNIST\n",
    "convnet_mnist = ConvNet(channel=1, num_classes=10, net_width=64, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='maxpooling', im_size=(224,224))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T00:48:35.406059Z",
     "start_time": "2024-10-16T00:48:35.383966Z"
    }
   },
   "id": "85b05db2e19515df",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize ConvNet-7 for MHIST\n",
    "convnet_mhist = ConvNet(channel=3, num_classes=2, net_width=64, net_depth=7, net_act='relu', net_norm='batchnorm', net_pooling='maxpooling', im_size=(224,224))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T00:50:40.886411Z",
     "start_time": "2024-10-16T00:50:40.873684Z"
    }
   },
   "id": "3e34d3d9f546c3",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_folder, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9214c82fb891e3d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": ".venv",
   "language": "python",
   "display_name": "Python (.venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
