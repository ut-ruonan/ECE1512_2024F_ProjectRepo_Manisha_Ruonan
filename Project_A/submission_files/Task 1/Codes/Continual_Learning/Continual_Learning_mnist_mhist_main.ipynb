{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8984b5-bf52-46fb-885b-2981ae0cf61b",
   "metadata": {},
   "source": [
    "# This code is the answer for \"Part 1 Question 4: Apply your synthetic small datasets to one of the machine learning applications.\"\n",
    "\n",
    "The synthetic small dataset is applied to continual leaning. So this case a model is trained and evaluated.\n",
    "code is constructed by refering from: https://www.kaggle.com/code/dlarionov/continual-learning-on-permuted-mnist\n",
    "\n",
    "Part II as of the experiment set up for continual learning in paper: \n",
    "DATASET CONDENSATION WITH GRADIENT MATCHING: Ref- \"*B. Zhao, K. R. Mopuri, and H. Bilen, “Dataset condensation with gradient matching,” arXiv preprint:2006.05929, 2021.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bd4ab-440d-44a6-81f3-953fb862ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import nbimporter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "from utils_continual import (\n",
    "    get_dataset,\n",
    "    get_loops,\n",
    "    # get_network,\n",
    "    get_eval_pool,\n",
    "    evaluate_synset,\n",
    "    get_default_convnet_setting,\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Covnet_continual import ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba255076-6465-486b-a913-b3b16666782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Device and hyperparameters\n",
    "device = torch.device(\"cpu\")\n",
    "train_bs = 64  # Training batch size\n",
    "test_bs = 2000  # Testing batch size\n",
    "lr = 0.1  # Learning rate\n",
    "gamma = 0.9  # Decay factor\n",
    "num_tasks = 5  # Number of tasks (adjust as needed for growing test sets)\n",
    "\n",
    "# Define the paths to your saved datasets\n",
    "base_path = 'C:\\\\Users\\\\mahagam3\\\\Documents\\\\ECE course\\\\Project A\\\\saved_datasets\\\\'  # Update this if necessary\n",
    "synthetic_dataset_mhist_path = os.path.join(base_path, 'synthetic_mhist_gray.pt')\n",
    "synthetic_dataset_mnist_path = os.path.join(base_path, 'synthetic_mnist.pt')\n",
    "mhist_test_loader_path = os.path.join(base_path, 'test_mhist.pt')\n",
    "mnist_test_loader_path = os.path.join(base_path, 'test_mnist.pt')\n",
    "\n",
    "# Load the datasets\n",
    "synthetic_dataset_mhist = torch.load(synthetic_dataset_mhist_path)\n",
    "synthetic_dataset_mnist = torch.load(synthetic_dataset_mnist_path)\n",
    "mhist_test_loader = torch.load(mhist_test_loader_path)\n",
    "mnist_test_loader = torch.load(mnist_test_loader_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf629e9f-d057-4552-a279-521bdb3e4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create test subset\n",
    "def get_test_subset(dataset, size):\n",
    "    size = min(size, len(dataset))\n",
    "    indices = np.random.choice(len(dataset), size, replace=False)\n",
    "    subset = Subset(dataset, indices)\n",
    "    return DataLoader(subset, batch_size=test_bs, shuffle=False)\n",
    "\n",
    "# MHIST configurations\n",
    "num_images_mhist = 100\n",
    "num_classes_mhist = 2\n",
    "# image_size_mhist = 224 * 224\n",
    "channels_mhist = 1\n",
    "\n",
    "# MNIST configurations\n",
    "num_images_mnist = 100\n",
    "num_classes_mnist = 10\n",
    "# image_size_mnist = 28 * 28\n",
    "channels_mnist = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02fd810-b329-422a-920d-9bcb3549fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize MHIST images \n",
    "transform_resize_mhist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 224x224\n",
    "    transforms.ToTensor()  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Resize MNIST images\n",
    "transform_resize_mnist = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 28x28\n",
    "    transforms.ToTensor()  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Synthetic data for MHIST\n",
    "synthetic_images_tensor_mhist = torch.rand(num_images_mhist, channels_mhist, 32, 32)\n",
    "synthetic_labels_tensor_mhist = torch.randint(0, num_classes_mhist, (num_images_mhist,))\n",
    "\n",
    "# Synthetic data for MNIST\n",
    "synthetic_images_tensor_mnist = torch.rand(num_images_mnist, channels_mnist, 32, 32)\n",
    "synthetic_labels_tensor_mnist = torch.randint(0, num_classes_mnist, (num_images_mnist,))\n",
    "\n",
    "# Create synthetic datasets\n",
    "synthetic_dataset_mhist = TensorDataset(synthetic_images_tensor_mhist, synthetic_labels_tensor_mhist)\n",
    "synthetic_dataset_mnist = TensorDataset(synthetic_images_tensor_mnist, synthetic_labels_tensor_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a6ea9-aa9e-44b7-9721-b39fa13372d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tasks for MHIST dataset\n",
    "tasks_mhist = [\n",
    "    (DataLoader(TensorDataset(\n",
    "        synthetic_images_tensor_mhist[:2000 * (i + 1)], \n",
    "        synthetic_labels_tensor_mhist[:2000 * (i + 1)]\n",
    "    ), batch_size=train_bs, shuffle=True), \n",
    "    TensorDataset(\n",
    "        synthetic_images_tensor_mhist[:2000 * (i + 1)], \n",
    "        synthetic_labels_tensor_mhist[:2000 * (i + 1)]\n",
    "    ))\n",
    "    for i in range(num_tasks)\n",
    "]\n",
    "\n",
    "# Create tasks for MNIST dataset\n",
    "tasks_mnist = [\n",
    "    (DataLoader(TensorDataset(\n",
    "        synthetic_images_tensor_mnist[:2000 * (i + 1)], \n",
    "        synthetic_labels_tensor_mnist[:2000 * (i + 1)]\n",
    "    ), batch_size=train_bs, shuffle=True), \n",
    "    TensorDataset(\n",
    "        synthetic_images_tensor_mnist[:2000 * (i + 1)], \n",
    "        synthetic_labels_tensor_mnist[:2000 * (i + 1)]\n",
    "    ))\n",
    "    for i in range(num_tasks)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df128b7-18ec-40dd-a883-48d275ed2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer Class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=100):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add_to_buffer(self, samples):\n",
    "        for sample in samples:\n",
    "            if len(sample) == 2:\n",
    "                input_tensor, target_tensor = sample\n",
    "                target_tensor = target_tensor.view(-1)  # Flatten to 1D tensor\n",
    "                self.buffer.append((input_tensor, target_tensor))\n",
    "\n",
    "                # Maintain buffer size\n",
    "                if self.buffer_size and len(self.buffer) > self.buffer_size:\n",
    "                    self.buffer.pop(0)  # Remove oldest sample if buffer exceeds size\n",
    "\n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        samples = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
    "        inputs = [sample[0] for sample in samples]\n",
    "        targets = [sample[1] for sample in samples]\n",
    "\n",
    "        inputs_tensor = torch.stack(inputs) if inputs else torch.empty(0)\n",
    "        targets_tensor = torch.stack(targets) if targets else torch.empty(0)\n",
    "\n",
    "        return list(zip(inputs_tensor, targets_tensor))\n",
    "\n",
    "# Function to create test subset\n",
    "def get_test_subset(dataset, size):\n",
    "    size = min(size, len(dataset))\n",
    "    indices = np.random.choice(len(dataset), size, replace=False)\n",
    "    subset = Subset(dataset, indices)\n",
    "    return DataLoader(subset, batch_size=test_bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9ac5d-8750-4ae6-a1e7-937d674d299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with Replay Buffer\n",
    "def train_model_with_replay(model, device, synthetic_loader, replay_buffer, optimizer, criterion, scheduler, replay_ratio=0.3):\n",
    "    model.train()\n",
    "    synthetic_data_iter = iter(synthetic_loader)\n",
    "    total_steps = len(synthetic_loader)\n",
    "    \n",
    "    for step in range(total_steps):\n",
    "        # Get current batch from synthetic data\n",
    "        try:\n",
    "            synthetic_batch = next(synthetic_data_iter)\n",
    "        except StopIteration:\n",
    "            synthetic_data_iter = iter(synthetic_loader)\n",
    "            synthetic_batch = next(synthetic_data_iter)\n",
    "        \n",
    "        # Sample from replay buffer\n",
    "        replay_batch = replay_buffer.sample_from_buffer(int(replay_ratio * len(synthetic_batch[0])))\n",
    "\n",
    "        # Add synthetic data to replay buffer\n",
    "        synthetic_samples = list(zip(synthetic_images_tensor_mhist.view(-1, 1, 32, 32), synthetic_labels_tensor_mhist))\n",
    "        replay_buffer.add_to_buffer(synthetic_samples)\n",
    "\n",
    "        # Combine synthetic data with replay data\n",
    "        if replay_batch:\n",
    "            # Ensure replay input tensors have the correct shape\n",
    "            replay_inputs = []\n",
    "            for x in replay_batch:\n",
    "                input_tensor = x[0]\n",
    "                if input_tensor.dim() == 3:\n",
    "                    input_tensor = input_tensor.unsqueeze(0)  # Convert to (1, H, W, C)\n",
    "                input_tensor = F.interpolate(input_tensor, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "                replay_inputs.append(input_tensor)\n",
    "\n",
    "            # Resize synthetic input\n",
    "            synthetic_input = synthetic_batch[0]\n",
    "            if synthetic_input.dim() == 3:\n",
    "                synthetic_input = synthetic_input.unsqueeze(0)\n",
    "            synthetic_input = F.interpolate(synthetic_input, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Concatenate inputs and targets\n",
    "            combined_inputs = torch.cat([synthetic_input] + replay_inputs, dim=0)\n",
    "            combined_targets = torch.cat([synthetic_batch[1]] + [x[1] for x in replay_batch], dim=0)\n",
    "        else:\n",
    "            combined_inputs, combined_targets = synthetic_batch\n",
    "\n",
    "        # Move to device\n",
    "        combined_inputs, combined_targets = combined_inputs.to(device), combined_targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(combined_inputs)\n",
    "        loss = criterion(output, combined_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840337c-429c-4fe8-b2f3-d00820dc28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function with growing test set\n",
    "def test_with_growing_test_set(model, device, test_dataset, num_tasks, tasks):\n",
    "    test_set_sizes = [2000, 4000]\n",
    "    metrics = []\n",
    "    replay_buffer = ReplayBuffer(buffer_size=5000)  # Replay buffer with a limit of 5000 samples\n",
    "\n",
    "    # Evaluate untrained model on test subsets\n",
    "    for size in test_set_sizes:\n",
    "        test_loader = get_test_subset(test_dataset, size)\n",
    "        test_acc = evaluate_model(model, test_loader)\n",
    "        print(f\"Test accuracy on {size} images: {test_acc:.2f}%\")\n",
    "        metrics.append(test_acc)\n",
    "    \n",
    "    for i in range(num_tasks):\n",
    "        print(f'Train on Task {i + 1}')\n",
    "        synthetic_loader, synthetic_data = tasks[i]  # Unpack the task\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "        \n",
    "        # Train the model on the current task using memory replay\n",
    "        train_model_with_replay(model, device, synthetic_loader, replay_buffer, optimizer, criterion, scheduler)\n",
    "        \n",
    "        # Add the synthetic data to the replay buffer\n",
    "        replay_buffer.add_to_buffer(synthetic_data.tensors)\n",
    "        \n",
    "        # Evaluate the model on different test set sizes\n",
    "        for size in test_set_sizes:\n",
    "            test_loader = get_test_subset(test_dataset, size)\n",
    "            test_acc = evaluate_model(model, test_loader)\n",
    "            print(f\"Test accuracy after training on Task {i + 1} with {size} test images: {test_acc:.2f}%\")\n",
    "            metrics.append(test_acc)\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "# Evaluate ConvNet3\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef3835-6ee8-4cfe-898a-6b11d2aa34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test models for both datasets\n",
    "all_experiments_mhist = []\n",
    "all_experiments_mnist = []\n",
    "num_experiments = 5\n",
    "\n",
    "def get_network(channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size=(32, 32)):\n",
    "    model = ConvNet(channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size)\n",
    "    return model\n",
    "\n",
    "convnet7_mhist = get_network(\n",
    "    channel=channels_mhist, \n",
    "    num_classes=num_classes_mhist, \n",
    "    net_width=128,  # Set your desired width\n",
    "    net_depth=7,    # This value was previously indicated\n",
    "    net_act='relu',  # Use the activation function of your choice\n",
    "    net_norm='batchnorm',  # Specify normalization (if needed)\n",
    "    net_pooling='maxpooling',  # Specify pooling method (if needed)\n",
    "    im_size=(32, 32)  # Ensure this matches your input size\n",
    ").to(device)\n",
    "\n",
    "convnet3_mnist = get_network(\n",
    "    channel=channels_mnist, \n",
    "    num_classes=num_classes_mnist, \n",
    "    net_width=128,  # Adjust if needed\n",
    "    net_depth=3,    # This value was previously indicated\n",
    "    net_act='relu',  # Use the activation function of your choice\n",
    "    net_norm='batchnorm',  # Specify normalization (if needed)\n",
    "    net_pooling='maxpooling',  # Specify pooling method (if needed)\n",
    "    im_size=(32, 32)  # Ensure this matches your input size\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7bf42-ff07-464f-8cc2-50b073d96bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in range(num_experiments):\n",
    "    print(f\"Running Experiment {experiment + 1} for MHIST\")\n",
    "    model_mhist = convnet7_mhist\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metrics_mhist = test_with_growing_test_set(model_mhist, device, synthetic_dataset_mhist, num_tasks, tasks_mhist)\n",
    "    all_experiments_mhist.append(metrics_mhist)\n",
    "\n",
    "    print(f\"Running Experiment {experiment + 1} for MNIST\")\n",
    "    model_mnist = convnet3_mnist\n",
    "    metrics_mnist = test_with_growing_test_set(model_mnist, device, synthetic_dataset_mnist, num_tasks, tasks_mnist)\n",
    "    all_experiments_mnist.append(metrics_mnist)\n",
    "\n",
    "\n",
    "# Convert results to numpy array for easy computation of mean and std\n",
    "all_experiments_mnist = np.array(all_experiments_mnist)\n",
    "\n",
    "mean_results_mnist = np.mean(all_experiments_mnist, axis=0)\n",
    "std_results_mnist = np.std(all_experiments_mnist, axis=0)\n",
    "\n",
    "# Print mean and std for each task\n",
    "for i, (mean, std) in enumerate(zip(mean_results_mnist, std_results_mnist)):\n",
    "    print(f\"Task {i + 1} - Mean accuracy MNIST: {mean:.4f}, Standard deviation MNIST: {std:.4f}\")\n",
    "\n",
    "\n",
    "# Convert results to numpy array for easy computation of mean and std\n",
    "all_experiments_mhist = np.array(all_experiments_mhist)\n",
    "\n",
    "mean_results_mhist = np.mean(all_experiments_mhist, axis=0)\n",
    "std_results_mhist = np.std(all_experiments_mhist, axis=0)\n",
    "\n",
    "# Print mean and std for each task\n",
    "for i, (mean, std) in enumerate(zip(mean_results_mhist, std_results_mhist)):\n",
    "    print(f\"Task {i + 1} - Mean accuracy MHSIT: {mean:.4f}, Standard deviation MHSIT: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179b128-04e8-4156-8452-d11f6d2bb5c8",
   "metadata": {},
   "source": [
    " ####################################################### Code Ends Here ######################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
