{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65a8f5527ccc8190"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Basic Concepts\n",
    "1. The purpose of using dataset distillation in this paper is to reduce the training costs while maintaining the high performance on various machine learning tasks. The authors introduce Dataset Distillation with Attention Maching (DataDAM) to condense large datasets into smaller synthetic dataset that retain the critical information, allowing models trained on the synthetic set to achieve similar accuracy as those trained on the full dataset.\n",
    "2.  The advantages are: (page 2)\n",
    "- Efficient end-to-end dataset distillation: This highlights the ability of DataDAM to closely approximate the distribution of the real dataset while keeping **computational costs low**.\n",
    "- Improved accuracy and scalability: DataDAM demonstrate the performance across multiple benchmark dataset and reduces the training costs by up to 100x, while also allowing for cross-architecture generation. This makes it more scalable and flexible for real-world application.\n",
    "- Enhancement of downstream application: DATADAM's distilled data improves memory efficiency in continual learning tasks and accelerates neural architectures search(NAS) by providing a more representative proxy dataset, enabling a faster and more efficient learning process.  \n",
    "3. The novelty includes: (page 2)\n",
    "- Multiple Randomly Initialized DNNs: DataDAM uses multiple randomly initialized deep neural networks to extract meaningful representations from both real and synthetic datasets, which is different from methods that rely on pre-trained models\n",
    "- Spatial attention matching (SAM): The SAM module align the most discriminative feature maps from real and synthetic datasets, reducing the gap between the dataset.\n",
    "- Last-Layer Feature Alignment: It reduces disparities in the last-layer feature distributions between the real and synthetic datasets by using a complementary loss as a regularizer, ensuring high-level abstract representations are similar.\n",
    "- Bias-Free Synthetic Data: The synthetic data generated by DataDAM does not introduce any bias, which is a significant improvement over prior methods, ensuring better generalization and performance.\n",
    "4. The methodology of DataDAM is centered on efficiently distilling datasets through attention matching: (page 4)\n",
    "- Initialization of Synthetic Dataset: The process starts by initializing a synthetic dataset, which can be done through random noise or by sampling real data.\n",
    "- Feature Extraction: Real and synthetic datasets are passed through randomly initialized deep neural networks, and features are extracted at multiple layers.\n",
    "- Spatial Attention Matching (SAM): Attention maps are computed for each layer, excluding the final layer. These attention maps focus on the most discriminative regions of the input image. \n",
    "- Loss Functions:\n",
    "    - SAM Loss (LSAM): This loss minimizes the distance between attention maps of real and synthetic datasets across layers.\n",
    "    - Maximum Mean Discrepancy Loss (LMMD): This complementary loss aligns the last-layer feature distributions of the two datasets, ensuring the high-level abstract information is captured.\n",
    "- Optimization: The synthetic dataset is optimized using a combination of the SAM loss and LMMD loss to minimize the difference between real and synthetic data.\n",
    "5. (page 8)\n",
    "- Continual Learning: DataDAMâ€™s ability to condense datasets efficiently makes it highly useful in continual learning scenarios, where a model must learn incrementally while preventing catastrophic forgetting. By using the distilled datasets as a replay buffer, DataDAM can significantly improve memory efficiency and performance in incremental learning tasks.\n",
    "- Neural Architecture Search (NAS): The synthetic datasets generated by DataDAM can serve as proxies in NAS tasks, allowing faster evaluation of model architectures. This leads to a significant reduction in computational costs and time during the model search process, making NAS more feasible in real-world applications."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b22ea2313b253dbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Data Distillation Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6968bdcc4e8c439"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import shutil \n",
    "import os \n",
    "\n",
    "annotations = pd.read_csv('mhist_dataset/annotations.csv')\n",
    "\n",
    "img_folder = 'mhist_dataset/images'\n",
    "train_folder = 'mhist_dataset/train/'\n",
    "test_folder = 'mhist_dataset/test/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T02:19:37.566400Z",
     "start_time": "2024-10-16T02:19:36.794317Z"
    }
   },
   "id": "7070b2acb6eef704",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have been successfully moved\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "for index, row in annotations.iterrows():\n",
    "    img_name = row['Image Name']\n",
    "    partition = row['Partition']\n",
    "    \n",
    "    src_path = os.path.join(img_folder, img_name)\n",
    "    if partition == 'train':\n",
    "        dst_path = os.path.join(train_folder, img_name)\n",
    "    else:\n",
    "        dst_path = os.path.join(test_folder, img_name)\n",
    "    \n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Images have been successfully moved\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:33:04.806730Z",
     "start_time": "2024-10-16T01:33:00.708942Z"
    }
   },
   "id": "56113c0d7dc783da",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been moved correctly and counts match!\n"
     ]
    }
   ],
   "source": [
    "actual_train_count = len(os.listdir(train_folder))\n",
    "actual_test_count = len(os.listdir(test_folder))\n",
    "\n",
    "expected_train_count = len(annotations[annotations['Partition'] == 'train'])\n",
    "expected_test_count = len(annotations[annotations['Partition'] == 'test'])\n",
    "\n",
    "if actual_train_count == expected_train_count and actual_test_count == expected_test_count:\n",
    "    print(\"All files have been moved correctly and counts match!\")\n",
    "else:\n",
    "    print(\"Warning: There is a mismatch between the files moved and the expected count!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T01:33:04.817588Z",
     "start_time": "2024-10-16T01:33:04.803827Z"
    }
   },
   "id": "4b90604094e2cca0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os \n",
    "import gzip \n",
    "import shutil\n",
    "\n",
    "dataset_dir = 'mnist_dataset/MNIST/raw'\n",
    "dataset_dest = 'mnist_dataset/MNIST/images'\n",
    "\n",
    "os.makedirs(dataset_dest, exist_ok=True)\n",
    "files_to_unzip = [f for f in os.listdir(dataset_dir) if f.endswith('.gz')]\n",
    "\n",
    "for file in files_to_unzip:\n",
    "    gz_path =os.path.join(dataset_dir, file)\n",
    "    unzipped_path = os.path.join(dataset_dest, file[:-3])\n",
    "    \n",
    "    with gzip.open(gz_path, 'rb') as f_in:\n",
    "        with open(unzipped_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T02:36:53.293083Z",
     "start_time": "2024-10-16T02:36:52.323258Z"
    }
   },
   "id": "cf3727d31f4de936",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ad9f45283d4e8b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T02:31:41.458940Z",
     "start_time": "2024-10-16T02:31:38.354293Z"
    }
   },
   "id": "4efd17a8d3f78422",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m      2\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor()\n\u001B[1;32m      3\u001B[0m ])\n\u001B[1;32m      5\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mImageFolder(root\u001B[38;5;241m=\u001B[39mtrain_folder, transform\u001B[38;5;241m=\u001B[39mtransform)\n\u001B[1;32m      6\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mImageFolder(root\u001B[38;5;241m=\u001B[39mtest_folder, transform\u001B[38;5;241m=\u001B[39mtransform)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_folder, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_folder, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T00:55:57.254750Z",
     "start_time": "2024-10-16T00:55:57.115373Z"
    }
   },
   "id": "5931daa181a604e8",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize ConvNet-3 for MNIST\n",
    "convnet_mnist = ConvNet(channel=1, num_classes=10, net_width=64, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='maxpooling', im_size=(224,224))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T00:48:35.406059Z",
     "start_time": "2024-10-16T00:48:35.383966Z"
    }
   },
   "id": "85b05db2e19515df",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize ConvNet-7 for MHIST\n",
    "convnet_mhist = ConvNet(channel=3, num_classes=2, net_width=64, net_depth=7, net_act='relu', net_norm='batchnorm', net_pooling='maxpooling', im_size=(224,224))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T00:50:40.886411Z",
     "start_time": "2024-10-16T00:50:40.873684Z"
    }
   },
   "id": "3e34d3d9f546c3",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_folder, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9214c82fb891e3d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "Python (myenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
