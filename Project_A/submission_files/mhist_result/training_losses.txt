Iteration 0: Loss = (0.031564127653837204, 0.03074103221297264, 0.0008230954990722239)
Iteration 1: Loss = (0.007008301094174385, 0.005936985835433006, 0.0010713152587413788)
Iteration 2: Loss = (0.03109886310994625, 0.030424509197473526, 0.0006743542617186904)
Iteration 3: Loss = (0.0108573529869318, 0.010079623199999332, 0.000777730077970773)
Iteration 4: Loss = (0.024459056556224823, 0.02379143051803112, 0.0006676253979094326)
Iteration 5: Loss = (0.0175029207020998, 0.016341133043169975, 0.0011617878917604685)
Iteration 6: Loss = (0.003689776174724102, 0.0027607842348515987, 0.0009289920562878251)
Iteration 7: Loss = (0.010217341594398022, 0.009450061246752739, 0.0007672801730223)
Iteration 8: Loss = (0.00523575721308589, 0.004473111592233181, 0.0007626456208527088)
Iteration 9: Loss = (0.004501349292695522, 0.003618099493905902, 0.0008832497987896204)
Iteration 10: Loss = (0.06806863099336624, 0.0672229751944542, 0.0008456527721136808)
Iteration 11: Loss = (0.051713235676288605, 0.05108114331960678, 0.0006320912507362664)
Iteration 12: Loss = (0.023970169946551323, 0.02297413907945156, 0.0009960313327610493)
Iteration 13: Loss = (0.03226575255393982, 0.03136669099330902, 0.0008990599308162928)
Iteration 14: Loss = (0.0033090119250118732, 0.0026048601139336824, 0.0007041518110781908)
Iteration 15: Loss = (0.010905931703746319, 0.009998627938330173, 0.0009073035325855017)
Iteration 16: Loss = (0.023646149784326553, 0.0228585172444582, 0.000787632423453033)
Iteration 17: Loss = (0.004832128994166851, 0.0040730079635977745, 0.0007591209141537547)
Iteration 18: Loss = (0.029374228790402412, 0.028642898425459862, 0.000731330830603838)
Iteration 19: Loss = (0.014326928183436394, 0.013709008693695068, 0.0006179193733260036)
Iteration 20: Loss = (0.0015477794222533703, 0.0004764613986480981, 0.0010713180527091026)
Iteration 21: Loss = (0.005573891568928957, 0.004378525074571371, 0.001195366494357586)
Iteration 22: Loss = (0.002733408473432064, 0.0015534828417003155, 0.0011799255153164268)
Iteration 23: Loss = (0.01650354266166687, 0.015447952784597874, 0.0010555896442383528)
Iteration 24: Loss = (0.05316990241408348, 0.05241026729345322, 0.000759633316192776)
Iteration 25: Loss = (0.0019916847813874483, 0.0007325891056098044, 0.0012590957339853048)
Iteration 26: Loss = (0.06265828013420105, 0.06172216311097145, 0.0009361190022900701)
Iteration 27: Loss = (0.010524807497859001, 0.009627512656152248, 0.0008972949581220746)
Iteration 28: Loss = (0.013980378396809101, 0.013054359704256058, 0.0009260183433070779)
Iteration 29: Loss = (0.004713166505098343, 0.0033513247035443783, 0.001361841568723321)
Iteration 30: Loss = (0.004113831557333469, 0.003090718062594533, 0.0010231132619082928)
Iteration 31: Loss = (0.02333037368953228, 0.02256421372294426, 0.0007661607232876122)
Iteration 32: Loss = (0.005586085841059685, 0.004721495322883129, 0.000864590285345912)
Iteration 33: Loss = (0.016898060217499733, 0.01592196151614189, 0.0009760991088114679)
Iteration 34: Loss = (0.007462237495929003, 0.006653653923422098, 0.0008085836889222264)
Iteration 35: Loss = (0.005451347678899765, 0.00470320601016283, 0.0007481419015675783)
Iteration 36: Loss = (0.028347143903374672, 0.027455609291791916, 0.0008915344951674342)
Iteration 37: Loss = (0.012400984764099121, 0.011822882108390331, 0.0005781021900475025)
Iteration 38: Loss = (0.07130265980958939, 0.07038208097219467, 0.0009205772657878697)
Iteration 39: Loss = (0.005620550364255905, 0.004591295961290598, 0.0010292544029653072)
Iteration 40: Loss = (0.0056686317548155785, 0.004638744052499533, 0.0010298877023160458)
Iteration 41: Loss = (0.0022811132948845625, 0.001229757210239768, 0.0010513560846447945)
Iteration 42: Loss = (0.007914770394563675, 0.006998829543590546, 0.0009159407345578074)
Iteration 43: Loss = (0.008440547622740269, 0.0076193539425730705, 0.0008211936219595373)
Iteration 44: Loss = (0.012000108137726784, 0.011023838073015213, 0.0009762703557498753)
Iteration 45: Loss = (0.009622400626540184, 0.00880537647753954, 0.0008170240325853229)
Iteration 46: Loss = (0.030935894697904587, 0.02996106445789337, 0.0009748298907652497)
Iteration 47: Loss = (0.005629346705973148, 0.004645424894988537, 0.0009839215781539679)
Iteration 48: Loss = (0.028901781886816025, 0.027921132743358612, 0.0009806493762880564)
Iteration 49: Loss = (0.0009817498503252864, 5.3642124839825556e-05, 0.0009281077655032277)
Iteration 50: Loss = (0.0034509177785366774, 0.00252844812348485, 0.0009224696550518274)
Iteration 51: Loss = (0.04173127934336662, 0.04084490239620209, 0.0008863784605637193)
Iteration 52: Loss = (0.038616519421339035, 0.03784703463315964, 0.0007694837404415011)
Iteration 53: Loss = (0.0033632938284426928, 0.0026661669835448265, 0.0006971269031055272)
Iteration 54: Loss = (0.02972627803683281, 0.02906813658773899, 0.0006581414490938187)
Iteration 55: Loss = (0.004692732356488705, 0.003777621779590845, 0.0009151105768978596)
Iteration 56: Loss = (0.008792414329946041, 0.007978258654475212, 0.0008141560247167945)
Iteration 57: Loss = (0.04270615801215172, 0.0416887030005455, 0.0010174540802836418)
Iteration 58: Loss = (0.03755636140704155, 0.03684848174452782, 0.0007078812923282385)
Iteration 59: Loss = (0.028522785753011703, 0.027811413630843163, 0.00071137142367661)
Iteration 60: Loss = (0.03508123755455017, 0.034316834062337875, 0.0007644030265510082)
Iteration 61: Loss = (0.0185332540422678, 0.017608944326639175, 0.000924309715628624)
Iteration 62: Loss = (0.002903579268604517, 0.0020798398181796074, 0.0008237393922172487)
Iteration 63: Loss = (0.003314848756417632, 0.0025158729404211044, 0.0007989758742041886)
Iteration 64: Loss = (0.011764169670641422, 0.010724067687988281, 0.0010401020990684628)
Iteration 65: Loss = (0.00473414221778512, 0.0037435104604810476, 0.0009906316408887506)
Iteration 66: Loss = (0.004403095692396164, 0.003475156147032976, 0.0009279393125325441)
Iteration 67: Loss = (0.0015937341377139091, 0.0005514624062925577, 0.0010422717314213514)
Iteration 68: Loss = (0.0029585957527160645, 0.002131211571395397, 0.0008273841231130064)
Iteration 69: Loss = (0.015824032947421074, 0.014681520871818066, 0.0011425125412642956)
Iteration 70: Loss = (0.0047635044902563095, 0.004007562529295683, 0.0007559421355836093)
Iteration 71: Loss = (0.019526192918419838, 0.01857229322195053, 0.0009539001621305943)
Iteration 72: Loss = (0.03552258387207985, 0.03423513099551201, 0.0012874527601525187)
Iteration 73: Loss = (0.02131660282611847, 0.020620573312044144, 0.0006960290484130383)
Iteration 74: Loss = (0.0025188929866999388, 0.001788466819562018, 0.0007304261671379209)
Iteration 75: Loss = (0.028818467631936073, 0.028085382655262947, 0.0007330845110118389)
Iteration 76: Loss = (0.018882358446717262, 0.017833076417446136, 0.0010492820292711258)
Iteration 77: Loss = (0.021355532109737396, 0.020197346806526184, 0.0011581857688724995)
Iteration 78: Loss = (0.0021324842236936092, 0.001400234643369913, 0.000732249696739018)
Iteration 79: Loss = (0.0678187757730484, 0.06701836735010147, 0.0008004084229469299)
Iteration 80: Loss = (0.008006519638001919, 0.006720670498907566, 0.0012858492555096745)
Iteration 81: Loss = (0.005506908521056175, 0.004591194447129965, 0.0009157142485491931)
Iteration 82: Loss = (0.01146808359771967, 0.010679337196052074, 0.0007887464598752558)
Iteration 83: Loss = (0.02407061867415905, 0.023336978629231453, 0.000733640044927597)
Iteration 84: Loss = (0.026118654757738113, 0.02522459253668785, 0.0008940616389736533)
Iteration 85: Loss = (0.021755145862698555, 0.020940354093909264, 0.000814791361335665)
Iteration 86: Loss = (0.0036153667606413364, 0.0026222974993288517, 0.0009930693777278066)
Iteration 87: Loss = (0.035733677446842194, 0.03492740914225578, 0.0008062683045864105)
Iteration 88: Loss = (0.00209986325353384, 0.0012215259484946728, 0.0008783373632468283)
Iteration 89: Loss = (0.024133220314979553, 0.02332032099366188, 0.0008128989720717072)
Iteration 90: Loss = (0.009498611092567444, 0.00872058141976595, 0.0007780297892168164)
Iteration 91: Loss = (0.0022728934418410063, 0.0015554996207356453, 0.000717393821105361)
Iteration 92: Loss = (0.012582771480083466, 0.011929545551538467, 0.0006532262195833027)
Iteration 93: Loss = (0.04163727909326553, 0.04070848226547241, 0.0009287981083616614)
Iteration 94: Loss = (0.011106327176094055, 0.01027875766158104, 0.0008275693980976939)
Iteration 95: Loss = (0.0037563107907772064, 0.002697326010093093, 0.0010589846642687917)
Iteration 96: Loss = (0.0022569741122424603, 0.001077565597370267, 0.0011794085148721933)
Iteration 97: Loss = (0.012737112119793892, 0.01190499309450388, 0.0008321190252900124)
Iteration 98: Loss = (0.0026151875499635935, 0.001492806593887508, 0.0011223809560760856)
Iteration 99: Loss = (0.021319305524230003, 0.02039210870862007, 0.0009271971648558974)
Iteration 100: Loss = (0.00322063360363245, 0.002290175762027502, 0.000930457899812609)
Iteration 101: Loss = (0.0010920612839981914, 0.00019522591901477426, 0.0008968353504315019)
Iteration 102: Loss = (0.03007822483778, 0.029263941571116447, 0.000814282801002264)
Iteration 103: Loss = (0.001833394169807434, 0.0012348892632871866, 0.0005985049065202475)
Iteration 104: Loss = (0.024689532816410065, 0.024081077426671982, 0.0006084558553993702)
Iteration 105: Loss = (0.013192722573876381, 0.011949652805924416, 0.0012430695351213217)
Iteration 106: Loss = (0.012492228299379349, 0.01189012173563242, 0.0006021061562933028)
Iteration 107: Loss = (0.0035556091461330652, 0.0025520646013319492, 0.001003544544801116)
Iteration 108: Loss = (0.030900729820132256, 0.0299370139837265, 0.000963716593105346)
Iteration 109: Loss = (0.006672820076346397, 0.005988568998873234, 0.0006842512520961463)
Iteration 110: Loss = (0.0073440964333713055, 0.006473218090832233, 0.0008708784589543939)
Iteration 111: Loss = (0.011873530223965645, 0.011089137755334377, 0.0007843927014619112)
Iteration 112: Loss = (0.038290925323963165, 0.03741791099309921, 0.0008730157860554755)
Iteration 113: Loss = (0.01089026965200901, 0.00942650344222784, 0.0014637659769505262)
Iteration 114: Loss = (0.025454655289649963, 0.024566736072301865, 0.0008879197994247079)
Iteration 115: Loss = (0.026416422799229622, 0.025473084300756454, 0.0009433376835659146)
Iteration 116: Loss = (0.017826762050390244, 0.016940008848905563, 0.0008867532014846802)
Iteration 117: Loss = (0.011696730740368366, 0.011024624109268188, 0.0006721065146848559)
Iteration 118: Loss = (0.03703731298446655, 0.03642093762755394, 0.0006163748912513256)
Iteration 119: Loss = (0.021407118067145348, 0.02062244527041912, 0.0007846731459721923)
Iteration 120: Loss = (0.0034408036153763533, 0.002788892248645425, 0.0006519114249385893)
Iteration 121: Loss = (0.0023946501314640045, 0.0018118929583579302, 0.0005827571148984134)
Iteration 122: Loss = (0.006244305521249771, 0.00545581616461277, 0.0007884892402216792)
Iteration 123: Loss = (0.010850084014236927, 0.009990688413381577, 0.000859395251609385)
Iteration 124: Loss = (0.018794406205415726, 0.018010558560490608, 0.0007838471210561693)
Iteration 125: Loss = (0.02969416230916977, 0.028886059299111366, 0.0008081034757196903)
Iteration 126: Loss = (0.016964254900813103, 0.015763932839035988, 0.0012003222946077585)
Iteration 127: Loss = (0.008415013551712036, 0.007517896126955748, 0.0008971175411716104)
Iteration 128: Loss = (0.004671694710850716, 0.003579299198463559, 0.0010923953959718347)
Iteration 129: Loss = (0.01465083658695221, 0.01370217278599739, 0.0009486642084084451)
Iteration 130: Loss = (0.04375229775905609, 0.04285862296819687, 0.0008936731028370559)
Iteration 131: Loss = (0.010935965925455093, 0.010156262665987015, 0.000779703026637435)
Iteration 132: Loss = (0.0013944873353466392, 7.92679056758061e-05, 0.0013152194442227483)
Iteration 133: Loss = (0.0029566586017608643, 0.002036280231550336, 0.0009203783702105284)
Iteration 134: Loss = (0.0013853623531758785, 0.0004603879642672837, 0.0009249743307009339)
Iteration 135: Loss = (0.010830817744135857, 0.009878571145236492, 0.00095224694814533)
Iteration 136: Loss = (0.004289121367037296, 0.003321828553453088, 0.0009672926971688867)
Iteration 137: Loss = (0.02300596609711647, 0.022196851670742035, 0.00080911535769701)
Iteration 138: Loss = (0.004579954780638218, 0.0036523081362247467, 0.0009276465862058103)
Iteration 139: Loss = (0.003376451088115573, 0.0026881080120801926, 0.0006883431342430413)
Iteration 140: Loss = (0.004822632763534784, 0.003913728520274162, 0.0009089043596759439)
Iteration 141: Loss = (0.05158447474241257, 0.050720371305942535, 0.0008641027379781008)
Iteration 142: Loss = (0.005820633377879858, 0.004910218995064497, 0.0009104144992306828)
Iteration 143: Loss = (0.01445023249834776, 0.013989628292620182, 0.0004606046131812036)
Iteration 144: Loss = (0.0176865067332983, 0.01670282892882824, 0.0009836785029619932)
Iteration 145: Loss = (0.023822594434022903, 0.02293260209262371, 0.0008899919921532273)
Iteration 146: Loss = (0.0036014278884977102, 0.002704381477087736, 0.0008970464114099741)
Iteration 147: Loss = (0.010036583058536053, 0.009059331379830837, 0.0009772517951205373)
Iteration 148: Loss = (0.007980930618941784, 0.007235433906316757, 0.0007454963633790612)
Iteration 149: Loss = (0.010733951814472675, 0.009929906576871872, 0.0008040451211854815)
Iteration 150: Loss = (0.03618680685758591, 0.03516646474599838, 0.0010203431593254209)
Iteration 151: Loss = (0.006139550358057022, 0.005007137078791857, 0.001132413512095809)
Iteration 152: Loss = (0.011503792367875576, 0.010807733982801437, 0.0006960582686588168)
Iteration 153: Loss = (0.003381245769560337, 0.002557717263698578, 0.000823528622277081)
Iteration 154: Loss = (0.01543338317424059, 0.014443698339164257, 0.0009896850679069757)
Iteration 155: Loss = (0.012372171506285667, 0.011773605830967426, 0.0005985660827718675)
Iteration 156: Loss = (0.006188455503433943, 0.0052266912534832954, 0.0009617641917429864)
Iteration 157: Loss = (0.005558410659432411, 0.004732505884021521, 0.0008259046007879078)
Iteration 158: Loss = (0.010808661580085754, 0.010014180094003677, 0.0007944817771203816)
Iteration 159: Loss = (0.0314704105257988, 0.03057645447552204, 0.0008939570980146527)
Iteration 160: Loss = (0.0034984347876161337, 0.0025025205686688423, 0.0009959142189472914)
Iteration 161: Loss = (0.02160104736685753, 0.020693868398666382, 0.0009071782696992159)
Iteration 162: Loss = (0.02917463704943657, 0.02833721786737442, 0.00083741988055408)
Iteration 163: Loss = (0.05697213113307953, 0.05608798936009407, 0.0008841430535539985)
Iteration 164: Loss = (0.0051230331882834435, 0.004245693329721689, 0.0008773400913923979)
Iteration 165: Loss = (0.007715361192822456, 0.007165197748690844, 0.000550163327716291)
Iteration 166: Loss = (0.009257534518837929, 0.008559003472328186, 0.0006985306390561163)
Iteration 167: Loss = (0.026163145899772644, 0.025474118068814278, 0.0006890270160511136)
Iteration 168: Loss = (0.008005954325199127, 0.007340478710830212, 0.0006654757307842374)
Iteration 169: Loss = (0.0020579411648213863, 0.0012616242747753859, 0.0007963168318383396)
Iteration 170: Loss = (0.003201959887519479, 0.0023158318363130093, 0.0008861280512064695)
Iteration 171: Loss = (0.032207150012254715, 0.03135968744754791, 0.0008474617497995496)
Iteration 172: Loss = (0.004911398980766535, 0.00406308751553297, 0.0008483112906105816)
Iteration 173: Loss = (0.006831740494817495, 0.006114418618381023, 0.0007173219928517938)
Iteration 174: Loss = (0.027314532548189163, 0.026448125019669533, 0.0008664076449349523)
Iteration 175: Loss = (0.04367659240961075, 0.04292803257703781, 0.0007485609967261553)
Iteration 176: Loss = (0.00597192021086812, 0.005104853305965662, 0.0008670667884871364)
Iteration 177: Loss = (0.010226347483694553, 0.009205246344208717, 0.0010211012559011579)
Iteration 178: Loss = (0.006970223039388657, 0.006249966565519571, 0.0007202565902844071)
Iteration 179: Loss = (0.014114784076809883, 0.013400107622146606, 0.0007146763382479548)
Iteration 180: Loss = (0.09018326550722122, 0.08939647674560547, 0.0007867866661399603)
Iteration 181: Loss = (0.054114654660224915, 0.05322181060910225, 0.00089284498244524)
Iteration 182: Loss = (0.0014297644374892116, 0.0006819836562499404, 0.0007477807812392712)
Iteration 183: Loss = (0.004153711721301079, 0.0028388691134750843, 0.0013148427242413163)
Iteration 184: Loss = (0.05392412096261978, 0.05314914137125015, 0.0007749794749543071)
Iteration 185: Loss = (0.03030218556523323, 0.029207363724708557, 0.0010948225390166044)
Iteration 186: Loss = (0.0030214465223252773, 0.002088958863168955, 0.0009324877755716443)
Iteration 187: Loss = (0.004011170007288456, 0.0033304495736956596, 0.0006807204918004572)
Iteration 188: Loss = (0.008192658424377441, 0.007311972323805094, 0.0008806863916106522)
Iteration 189: Loss = (0.008514389395713806, 0.00764932157471776, 0.000865067879203707)
Iteration 190: Loss = (0.003836757503449917, 0.0025749611668288708, 0.001261796336621046)
Iteration 191: Loss = (0.004741825629025698, 0.003805177751928568, 0.0009366480517201126)
Iteration 192: Loss = (0.04384854808449745, 0.04300622642040253, 0.0008423200924880803)
Iteration 193: Loss = (0.00542535912245512, 0.004856280516833067, 0.0005690787802450359)
Iteration 194: Loss = (0.04296881705522537, 0.04202444478869438, 0.000944373314268887)
Iteration 195: Loss = (0.03624531254172325, 0.03525242209434509, 0.0009928890503942966)
Iteration 196: Loss = (0.008719377219676971, 0.007823163643479347, 0.0008962136344052851)
Iteration 197: Loss = (0.019366947934031487, 0.018705062568187714, 0.0006618858315050602)
Iteration 198: Loss = (0.0029657920822501183, 0.0017578101251274347, 0.0012079819571226835)
Iteration 199: Loss = (0.0078002032823860645, 0.007043318822979927, 0.0007568843429908156)
